{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfafb5a4",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "\n",
    "The basins are filtered to the 296 study basins in the `S2-Join_CA_Data` notebook. The partitions for Experiment A and Experiment E are also defined in that notebook. The following code defines the partitions for the remaining Experiments B, C, and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc816c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import yaml\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get available gages\n",
    "files = os.listdir('Data/basins/time_series')\n",
    "gages = []\n",
    "for f in files:\n",
    "    gages.append(f.split('.')[0])\n",
    "    \n",
    "att_df = pd.read_csv('Data/basins/attributes_lat_lon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8413574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random groups\n",
    "all_basins = pd.read_csv('Data/basins/e_basins.txt', header=None)\n",
    "all_basins['group_n'] = np.random.randint(3, size=len(all_basins))\n",
    "\n",
    "group_a = all_basins.loc[all_basins.group_n == 0, 0].tolist()\n",
    "group_b = all_basins.loc[all_basins.group_n == 1, 0].tolist()\n",
    "group_c = all_basins.loc[all_basins.group_n == 2, 0].tolist()\n",
    "\n",
    "att_df['random_group'] = np.nan\n",
    "for i in all_basins[0]:\n",
    "    g = all_basins.loc[all_basins[0] == i, 'group_n'].item()\n",
    "    att_df.loc[att_df['index'] == i, 'random_group'] = g\n",
    "\n",
    "# att_df.dropna(subset=['random_group'], axis=0).to_csv('Data/basins/attributes_lat_lon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups based on water balance\n",
    "att_df['wb'] = np.nan\n",
    "\n",
    "for f in files:\n",
    "    nc_xr = xr.open_dataset('Data/basins/time_series/' + f)\n",
    "    nc_df = nc_xr.to_dataframe()\n",
    "\n",
    "    q_p = nc_df.q_cms.mean() / nc_df.total_precipitation__sum__era5l_daily.mean()\n",
    "    pe_p = nc_df.potential_evaporation__sum__era5l_daily.mean()/ nc_df.total_precipitation__sum__era5l_daily.mean()\n",
    "    wb = q_p - (1 - pe_p)\n",
    "\n",
    "    att_df.loc[att_df['index'] == int(f.split('.')[0]), 'wb'] = wb\n",
    "    \n",
    "att_df['wb_group'] = np.nan\n",
    "att_df.loc[att_df.wb > -100, 'wb_group'] = 'a'\n",
    "att_df.loc[(att_df.wb <= -100) & (att_df.wb >= -2000), 'wb_group'] = 'b'\n",
    "# att_df.loc[(att_df.wb < -2000) & (att_df.wb > -4000), 'wb_group'] = 'c'\n",
    "att_df.loc[att_df.wb < -2000, 'wb_group'] = 'c'\n",
    "\n",
    "att_df.wb_group.value_counts()\n",
    "# att_df.to_csv('Data/basins/attributes_lat_lon.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c954d",
   "metadata": {},
   "source": [
    "### Hyperparameter Grid Search\n",
    "The following code generates the `yml` files used for the hyperparameter grid search for each experiment. The following parameters are explored:\n",
    "\n",
    "- hidden size: 32, 121, 256\n",
    "- sequence length: 10, 90, 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write hyperparameter ymls\n",
    "\n",
    "subgroups = ['e_group_dam', 'e_group_flashy', 'e_group_natural']\n",
    "hs = [32, 121, 256]\n",
    "sl = [10, 90, 365]\n",
    "\n",
    "for s in subgroups:\n",
    "    with open(\"Data/hyperparameters/sample_ca_parameters.yml\", 'r') as stream:\n",
    "        run_config = yaml.safe_load(stream)\n",
    "\n",
    "    for seq in sl:\n",
    "        for hid_sz in hs:\n",
    "            run_config['train_basin_file'] = 'Data/basins/' + s + '.txt'\n",
    "            run_config['validation_basin_file'] = 'Data/basins/' + s + '.txt'\n",
    "            run_config['test_basin_file'] = 'Data/basins/' + s + '.txt'\n",
    "            run_config['seq_length'] = seq\n",
    "            run_config['hidden_size'] = hid_sz\n",
    "\n",
    "            sub_name = 'Data/hyperparameters/e_ca_ymls/' + s + '_sl_' + str(seq) + '_hs_' + str(hid_sz) + '.yml'\n",
    "            ex_name = s + '_sl_' + str(seq) + '_hs_' + str(hid_sz)\n",
    "            run_config['experiment_name'] = ex_name\n",
    "\n",
    "            # save updated yml\n",
    "            with io.open(sub_name, 'w', encoding='utf8') as outfile:\n",
    "                yaml.dump(run_config, outfile, default_flow_style=False, allow_unicode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-vote] *",
   "language": "python",
   "name": "conda-env-miniconda3-vote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
