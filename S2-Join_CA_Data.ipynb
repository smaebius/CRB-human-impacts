{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e260b3",
   "metadata": {},
   "source": [
    "### Join CA Data\n",
    "The Climate/Anthropogenic data accessed from Google Earth Engine in the `S1-Load_CA_Data` notebook is joined to the streamflow data in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = gpd.read_file('Data/basins/attributes.gpkg')\n",
    "\n",
    "# join monthly water recurrence data\n",
    "df = pd.read_csv('Data/gee/JRC-GSW1_4-MonthlyRecurrence__monthly_recurrence.csv')\n",
    "df['month'] = df['system:index'].apply(lambda x: int(x.split('_')[2]))\n",
    "\n",
    "static_df['jan_recurrence'] = np.nan\n",
    "static_df['feb_recurrence'] = np.nan\n",
    "static_df['mar_recurrence'] = np.nan\n",
    "static_df['apr_recurrence'] = np.nan\n",
    "static_df['may_recurrence'] = np.nan\n",
    "static_df['jun_recurrence'] = np.nan\n",
    "static_df['jul_recurrence'] = np.nan\n",
    "static_df['aug_recurrence'] = np.nan\n",
    "static_df['sep_recurrence'] = np.nan\n",
    "static_df['oct_recurrence'] = np.nan\n",
    "static_df['nov_recurrence'] = np.nan\n",
    "static_df['dec_recurrence'] = np.nan\n",
    "\n",
    "month_dict = {0:'jan_recurrence', 1:'feb_recurrence', 2:'mar_recurrence',\n",
    "              3:'apr_recurrence', 4:'may_recurrence', 5:'jun_recurrence',\n",
    "              6:'jul_recurrence', 7:'aug_recurrence', 8:'sep_recurrence',\n",
    "              9:'oct_recurrence', 10:'nov_recurrence', 11:'dec_recurrence'}\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    df_col = month_dict[r.month]\n",
    "    static_df.loc[vote_df['index'] == r['index'], df_col] = r['mean']\n",
    "    \n",
    "static_df.to_csv('Data/basins/attributes/all.csv', index=False)\n",
    "\n",
    "vote_df = pd.read_csv('Data/basins/attributes/all.csv')\n",
    "files = os.listdir('Data/basins/time_series')\n",
    "\n",
    "for i in files:\n",
    "    i = str(i.split('.')[0])\n",
    "    nc_xr = xr.open_dataset('Data/basins/time_series/' + i + '.nc')\n",
    "    nc_df = nc_xr.to_dataframe()\n",
    "    nc_df['month'] = nc_df.index.month\n",
    "    nc_df['recurrence'] = nc_df.apply(lambda x: vote_df.loc[vote_df['index'] == int(i), month_dict[int(x.month) - 1]].item(), axis=1)\n",
    "    nc_df.to_xarray().to_netcdf('Data/basins/time_series/' + i + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join LCMAP time series\n",
    "df = pd.read_csv('Data/gee/projects-sat-io-open-datasets-LCMAP-LCPRI__b1.csv')\n",
    "df['year'] = df['system:index'].apply(lambda x: int(x.split('_')[2]))\n",
    "\n",
    "hist_dict = {1: 'developed', 2: 'cropland', 3: 'grass_shrub', 4: 'tree_cover',\n",
    "             5: 'water', 6: 'wetland', 7: 'ice_snow', 8: 'barren'}\n",
    "\n",
    "df['developed'] = np.nan\n",
    "df['cropland'] = np.nan\n",
    "df['grass_shrub'] = np.nan\n",
    "df['tree_cover'] = np.nan\n",
    "df['water'] = np.nan\n",
    "df['wetland'] = np.nan\n",
    "df['ice_snow'] = np.nan\n",
    "df['barren'] = np.nan\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    hist_list = r.histogram.split('{')[1].split('}')[0].split(',')\n",
    "    \n",
    "    for i in hist_list:\n",
    "        df_var = hist_dict[int(i.split('=')[0])]\n",
    "        df.loc[df['system:index'] == r['system:index'], df_var] = float(i.split('=')[1])\n",
    "        \n",
    "df = df.fillna(0.0)\n",
    "\n",
    "files = os.listdir('Data/basins/time_series')\n",
    "\n",
    "for f in files:\n",
    "    nc_xr = xr.open_dataset('Data/basins/time_series/'+f)\n",
    "    f_gage = f.split('.')[0]\n",
    "    df_temp = df.loc[df['index'] == int(f_gage)][['index', 'year', 'developed', 'cropland', 'grass_shrub', 'tree_cover',\n",
    "                                                     'water', 'wetland', 'ice_snow', 'barren']]\n",
    "    df_temp.index = pd.to_datetime(pd.DataFrame({'year': df_temp.year, 'month': 1, 'day': 1}))\n",
    "    df_temp = df_temp.resample('D').mean().fillna(method='ffill')\n",
    "    df_temp.index.rename('date',inplace=True)\n",
    "    xr_gage = df_temp.to_xarray()\n",
    "    nc_xr = nc_xr.merge(xr_gage, join='left')\n",
    "    nc_xr.to_netcdf('Data/basins/time_series/'+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join rangeland agriculture data\n",
    "df = pd.read_csv('Data/gee/projects-rap-data-365417-assets-vegetation-cover-v3.csv')\n",
    "df['year'] = df['system:index'].apply(lambda x: int(x.split('_')[0]))\n",
    "\n",
    "hist_dict = {'AFG_mean': 'annual_forbs_grass', 'PFG_mean': 'perenn_forbs_grass', 'SHR_mean': 'shrubs',\n",
    "             'TRE_mean': 'trees', 'BGR_mean': 'bare_ground'}\n",
    "\n",
    "for i in hist_dict:\n",
    "    df[hist_dict[i]] = df[i]\n",
    "    \n",
    "files = os.listdir('Data/basins/time_series')\n",
    "\n",
    "for f in files:\n",
    "    nc_xr = xr.open_dataset('Data/basins/time_series/'+f)\n",
    "    f_gage = f.split('.')[0]\n",
    "    df_temp = df.loc[df['index'] == int(f_gage)][['index', 'year', 'annual_forbs_grass', 'perenn_forbs_grass', 'shrubs', 'trees', 'bare_ground']]\n",
    "    df_temp.index = pd.to_datetime(pd.DataFrame({'year': df_temp.year, 'month': 1, 'day': 1}))\n",
    "    df_temp = df_temp.resample('D').mean().fillna(method='ffill')\n",
    "    df_temp.index.rename('date',inplace=True)\n",
    "    xr_gage = df_temp.to_xarray()\n",
    "    nc_xr = nc_xr.merge(xr_gage, join='left')\n",
    "    nc_xr.to_netcdf('Data/basins/time_series/'+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join random variable data\n",
    "files = os.listdir('Data/basins/time_series')\n",
    "\n",
    "for f in files:\n",
    "    nc_df = xr.open_dataset('Data/basins/time_series/'+f).to_dataframe()\n",
    "    nc_df['rand_n'] = np.random.rand(len(nc_df))\n",
    "    nc_df.to_xarray().to_netcdf('Data/basins/time_series/'+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64029cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join hydro atlas data\n",
    "for i in static_df['index']:\n",
    "    nc_df = xr.open_dataset('Data/basins/time_series/'+str(i)+'.nc').to_dataframe()\n",
    "    aet, pet, pre, cmi, pnv = [], [], [], [], []\n",
    "    for n in range(1, 12+1):\n",
    "        name = 'ha_aet_mm_s' + str(n).zfill(2)\n",
    "        aet.append(static_df.loc[static_df['index'] == int(i), name].item())\n",
    "        \n",
    "        name = 'ha_pet_mm_s' + str(n).zfill(2)\n",
    "        pet.append(static_df.loc[static_df['index'] == int(i), name].item())\n",
    "        \n",
    "        name = 'ha_pre_mm_s' + str(n).zfill(2)\n",
    "        pre.append(static_df.loc[static_df['index'] == int(i), name].item())\n",
    "        \n",
    "        name = 'ha_cmi_ix_s' + str(n).zfill(2)\n",
    "        cmi.append(static_df.loc[static_df['index'] == int(i), name].item())\n",
    "        \n",
    "        name = 'ha_pnv_pc_s' + str(n).zfill(2)\n",
    "        pnv.append(static_df.loc[static_df['index'] == int(i), name].item())\n",
    "\n",
    "    aet_df = pd.DataFrame({'month':list(np.arange(1.0, 13.0)),\n",
    "                           'aet':aet, 'pet':pet, 'pre':pre, 'cmi':cmi, 'pnv':pnv})\n",
    "\n",
    "    nc_df['aet'] = np.nan\n",
    "    nc_df['pet'] = np.nan\n",
    "    nc_df['pre'] = np.nan\n",
    "    nc_df['cmi'] = np.nan\n",
    "    nc_df['pnv'] = np.nan\n",
    "    for idx, r in aet_df.iterrows():\n",
    "        m = r.month\n",
    "        nc_df.loc[nc_df.month == m, 'aet'] = r.aet\n",
    "        nc_df.loc[nc_df.month == m, 'pet'] = r.pet\n",
    "        nc_df.loc[nc_df.month == m, 'pre'] = r.pre\n",
    "        nc_df.loc[nc_df.month == m, 'cmi'] = r.cmi\n",
    "        nc_df.loc[nc_df.month == m, 'pnv'] = r.pnv\n",
    "\n",
    "    nc_df.to_xarray().to_netcdf('Data/basins/time_series/'+str(i)+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3553bb0",
   "metadata": {},
   "source": [
    "## Filter USGS Gauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into natural/undammed + post dam\n",
    "basins = os.listdir('Data/basins/time_series/')\n",
    "\n",
    "for i in static_df['index']:\n",
    "    if str(i) + '.nc' in basins:\n",
    "        nc_xr = xr.open_dataset(\"Data/basins/time_series/\"+str(i)+\".nc\")\n",
    "        if static_df.loc[static_df['index'] == i].nid_name.item() == '':\n",
    "            # undammed\n",
    "            nc_xr.to_netcdf('Data/basins/natural/'+str(i)+'.nc')\n",
    "        else:\n",
    "            yr = int(static_df.loc[static_df['index'] == i, 'yr_completed'].item())\n",
    "            # pre dam\n",
    "            if datetime.datetime(nc_xr.date.min().dt.year.to_numpy(), 1, 1) < datetime.datetime(int(yr), 1, 1):\n",
    "                nc_xr_pre = nc_xr.sel(date=slice(nc_xr.date.min().to_numpy(), str(yr)+'-01-01'))\n",
    "                nc_xr_pre.to_netcdf('Data/basins/natural/'+str(i)+'.nc')\n",
    "\n",
    "            # post dam\n",
    "            if datetime.datetime(nc_xr.date.max().dt.year.to_numpy(), 1, 1) > datetime.datetime(int(yr), 1, 1):\n",
    "                nc_xr_post = nc_xr.sel(date=slice(str(yr)+'-01-01', nc_xr.date.max().to_numpy()))\n",
    "                nc_xr_post.to_netcdf('Data/basins/dam/'+str(i)+'.nc')\n",
    "\n",
    "natural = os.listdir('Data/basins/natural')\n",
    "dam = os.listdir('Data/basins/dam')\n",
    "\n",
    "natural_basins, dam_basins = [], []\n",
    "\n",
    "for b in all_basins[0]:\n",
    "    basin = str(b) + '.nc'\n",
    "    \n",
    "    if basin in natural:\n",
    "        nc_df = xr.open_dataset('Data/basins/natural/' + basin).to_dataframe()\n",
    "        if len(nc_df['2000-10-01':'2020-10-01'].dropna(subset='q_cms')) >= 365 * 12:\n",
    "            natural_basins.append(b)\n",
    "\n",
    "    if basin in dam:\n",
    "        nc_df = xr.open_dataset('Data/basins/dam/' + basin).to_dataframe()\n",
    "        if len(nc_df['2000-10-01':'2020-10-01'].dropna(subset='q_cms')) >= 365 * 12:\n",
    "            dam_basins.append(b)\n",
    "\n",
    "flashy_basins, natural_b, dam_b = [], [], []\n",
    "for i in natural_basins:\n",
    "    basin = str(i) + '.nc'\n",
    "    nc_df = xr.open_dataset('Data/basins/natural/' + basin).to_dataframe()\n",
    "    nc_df = nc_df['2000-10-01':'2020-10-01']\n",
    "    if len(nc_df.loc[nc_df.q_cms < 0.01])/len(nc_df) >= 0.4:\n",
    "        flashy_basins.append(i)\n",
    "#         plt.plot(nc_df.q_cms)\n",
    "#         plt.show()\n",
    "    else:\n",
    "        natural_b.append(i)\n",
    "\n",
    "for i in dam_basins:\n",
    "    basin = str(i) + '.nc'\n",
    "    nc_df = xr.open_dataset('Data/basins/dam/' + basin).to_dataframe()\n",
    "    nc_df = nc_df['2000-10-01':'2020-10-01']\n",
    "    if len(nc_df.loc[nc_df.q_cms < 0.01])/len(nc_df) >= 0.4:\n",
    "        flashy_basins.append(i)\n",
    "#         plt.plot(nc_df.q_cms)\n",
    "#         plt.show()\n",
    "    else:\n",
    "        dam_b.append(i)\n",
    "        \n",
    "static_df['split_group'] = np.nan\n",
    "\n",
    "for i in all_basins[0]:\n",
    "    if i in flashy_basins:\n",
    "        att_df.loc[att_df['index'] == i, 'split_group'] = 'flashy'\n",
    "    if i in natural_b:\n",
    "        att_df.loc[att_df['index'] == i, 'split_group'] = 'natural'\n",
    "    if i in dam_b:\n",
    "        att_df.loc[att_df['index'] == i, 'split_group'] = 'dam'\n",
    "        \n",
    "static_df.to_csv('Data/basins/attributes/all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-vote] *",
   "language": "python",
   "name": "conda-env-miniconda3-vote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
