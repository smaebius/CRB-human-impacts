{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ddc2ed",
   "metadata": {},
   "source": [
    "### Training and Testing with Neuralhydrology\n",
    "The following code uses the [`neuralhydrology`](https://neuralhydrology.readthedocs.io/en/latest/) python library to train and test LSTM streamflow models. An example of the code for the regional LSTM is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.nh_run import start_run, eval_run\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter grid search training for Experiment E with CA Data\n",
    "ymls = os.listdir('Data/hyperparameters/e_ca_ymls/')\n",
    "ymls_done = ['e_group_dam_sl_10_hs_121.yml',\n",
    "             'e_group_dam_sl_10_hs_256.yml',\n",
    "             'e_group_dam_sl_10_hs_32.yml',\n",
    "             'e_group_dam_sl_365_hs_121.yml',\n",
    "             'e_group_dam_sl_365_hs_256.yml',\n",
    "             'e_group_dam_sl_365_hs_32.yml',\n",
    "             'e_group_dam_sl_90_hs_121.yml']\n",
    "nse = []\n",
    "runs = []\n",
    "\n",
    "for y in ymls:\n",
    "    print(y)\n",
    "    if y not in ymls_done:\n",
    "        y_path = 'Data/hyperparameters/e_ca_ymls/' + y\n",
    "\n",
    "        start_run(config_file=Path(y_path))\n",
    "\n",
    "# eval_run(run_dir=Path('runs/e_group_dam_sl_90_hs_256_0802_134525'), period='test', epoch=50)\n",
    "# eval_run(run_dir=Path('runs/e_group_flashy_sl_90_hs_256_0802_125052'), period='test', epoch=50)\n",
    "# eval_run(run_dir=Path('runs/e_group_natural_sl_90_hs_32_0802_130259'), period='test', epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter grid search evaluating for Experiment A\n",
    "nse = []\n",
    "run_path = \"runs/a_basins_2201_142415\"\n",
    "run_dir = Path(run_path)\n",
    "\n",
    "\n",
    "for n in range(10, 26):\n",
    "    print('epoch: ', n)\n",
    "    # eval_run(run_dir=run_dir, period=\"validation\", epoch=n)\n",
    "\n",
    "    if n < 100:\n",
    "        epoch_name = \"model_epoch0\" + str(n)\n",
    "    \n",
    "    else:\n",
    "        epoch_name = \"model_epoch\" + str(n)\n",
    "\n",
    "    with open(run_dir / \"validation\" / epoch_name / \"validation_results.p\", \"rb\") as fp:\n",
    "        results = pickle.load(fp)\n",
    "\n",
    "    nses = []\n",
    "    for k in results.keys():\n",
    "        if 'NSE' in results[k]['1D']:\n",
    "            nses.append(results[k]['1D']['NSE'])\n",
    "\n",
    "    # get the median NSE\n",
    "    nse.append(np.nanmean(nses))\n",
    "\n",
    "epoch_n = np.array(nse).argmax() + 10\n",
    "print('Optimal epoch: ', epoch_n, 'NSE: ', np.max(nse))\n",
    "\n",
    "eval_run(run_dir=run_dir, period=\"test\", epoch=epoch_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca6570",
   "metadata": {},
   "source": [
    "### SHAP Feature Importance\n",
    "\n",
    "This section provides the sample code to compute the SHAP values for a basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184de257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from neuralhydrology.evaluation.utils import load_scaler\n",
    "from neuralhydrology.modelzoo.cudalstm import CudaLSTM\n",
    "from neuralhydrology.utils.config import Config\n",
    "from neuralhydrology.datasetzoo import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd689bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin_id = \"-14895675224\"\n",
    "seq_len = 90\n",
    "\n",
    "config_file=Path(\"Data/hyperparameters/exp_e_ca_ymls/e_group_natural_sl_90_hs_121.yml\")\n",
    "cudalstm_config = Config(config_file)\n",
    "\n",
    "# create a new model instance with random weights\n",
    "cuda_lstm = CudaLSTM(cfg=cudalstm_config)\n",
    "\n",
    "run_dir = Path(\"Data/runs/e_group_natural_sl_90_hs_121_2002_215342\")\n",
    "\n",
    "# load the trained weights into the new model.\n",
    "model_path = run_dir / 'model_epoch050.pt'\n",
    "model_weights = torch.load(str(model_path), map_location='cpu')\n",
    "cuda_lstm.load_state_dict(model_weights)\n",
    "\n",
    "# load the dataset\n",
    "scaler = load_scaler(run_dir)\n",
    "dataset = get_dataset(cudalstm_config, basin=basin_id, is_train=False, period='train', scaler=scaler)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "dataset_test = get_dataset(cudalstm_config, basin=basin_id, is_train=False, period='test', scaler=scaler)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=dataset.collate_fn)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "sample = next(iter(dataloader))\n",
    "\n",
    "################################################################################\n",
    "\n",
    "M = 1000\n",
    "counter = 0\n",
    "n_features = sample['x_d'].shape[2]\n",
    "n = sample['x_d'].shape[1]\n",
    "features = list(range(n_features))\n",
    "\n",
    "shapley_ts = {}\n",
    "for i in range(0, n_features):\n",
    "    for k in range(0, n):\n",
    "        key = str(i) + '_' + str(k)\n",
    "        shapley_ts[key] = []\n",
    "\n",
    "for sample_test in dataloader_test:\n",
    "    if counter > 731:\n",
    "        break\n",
    "    counter += 1\n",
    "    if counter > 718:\n",
    "        print('Sample: ', counter)\n",
    "        # feature_dict = {}\n",
    "        x = sample_test['x_d']\n",
    "        for j in features:\n",
    "            marginal_contributions = {}\n",
    "            phi_j_x = []\n",
    "\n",
    "            for i in range(0, n):\n",
    "                marginal_contributions[i] = []\n",
    "            \n",
    "            feature_idxs = list(range(n_features))\n",
    "            feature_idxs.remove(j)\n",
    "            \n",
    "            for _ in range(M):\n",
    "                z = next(iter(dataloader))['x_d']\n",
    "                x_idx = random.sample(feature_idxs, min(max(int(0.2*n_features), random.choice(feature_idxs)), int(0.8*n_features)))\n",
    "                z_idx = [idx for idx in feature_idxs if idx not in x_idx]\n",
    "\n",
    "                # construct two new instances\n",
    "                x_plus_j = np.array([(x[:,:,i][0]).numpy() if i in x_idx + [j] else (z[:,:,i][0]).numpy() for i in range(n_features)]).reshape((seq_len, n_features))\n",
    "                x_minus_j = np.array([(z[:,:,i][0]).numpy() if i in x_idx + [j] else (x[:,:,i][0]).numpy() for i in range(n_features)]).reshape((seq_len, n_features))\n",
    "                x_plus_j = np.expand_dims(x_plus_j, 0)\n",
    "                x_minus_j = np.expand_dims(x_minus_j, 0)\n",
    "                \n",
    "                # calculate marginal contribution\n",
    "                s_minus_j = {'x_d': torch.Tensor(x_minus_j).cpu(), 'y': sample_test['y'], 'date': sample_test['date'],\n",
    "                            'x_s': sample_test['x_s']}\n",
    "                y_hat_minus_j = cuda_lstm(s_minus_j)['y_hat']\n",
    "                \n",
    "                s_plus_j = {'x_d': torch.Tensor(x_plus_j).cpu(), 'y': sample_test['y'], 'date': sample_test['date'],\n",
    "                            'x_s': sample_test['x_s']}\n",
    "                y_hat_plus_j = cuda_lstm(s_plus_j)['y_hat']\n",
    "                \n",
    "                marginal_contribution = y_hat_plus_j - y_hat_minus_j\n",
    "\n",
    "                for k in range(0, n):\n",
    "                    marginal_contributions[k].append(float(marginal_contribution[:,k,:][0][0].detach().cpu()))\n",
    "\n",
    "            for l in range(0, n):\n",
    "                phi_j_x.append(sum(marginal_contributions[l])/len(marginal_contributions[l]))\n",
    "\n",
    "            print(f\"Shaply value for feature {j}:\", phi_j_x)\n",
    "\n",
    "            for i in shapley_ts.keys():\n",
    "                a, b = i.split('_')[0], i.split('_')[1]\n",
    "                if a == str(j):\n",
    "                    shapley_ts[i].append(phi_j_x[int(b)])\n",
    "\n",
    "        shap_name = 'Data/shaps/shapley_ts_' + basin_id + '_' + str(counter) + '.csv'\n",
    "        pd.DataFrame(dict([(k,pd.Series(v)) for k,v in shapley_ts.items() ])).to_csv(shap_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-vote] *",
   "language": "python",
   "name": "conda-env-miniconda3-vote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
