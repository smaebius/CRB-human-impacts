{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e8c6cf",
   "metadata": {},
   "source": [
    "### Download Data Using VotE\n",
    "Data is accessed using the `VotE` python package. All of the available gauges in the CRB are downloaded and formatted into `neuralhydrology` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from VotE.streamflow import export_streamflow as es\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d093fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_col = gpd.read_file(\"Data/CRB_basin.shp\")\n",
    "CRB_gages = es.gage_selector({\"within\": geo_col.geometry[0],\"gap_free\": False})\n",
    "CRB_watersheds = es.get_gages(CRB_gages, geom_col='basin_geom_vote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time series and attributes directories\n",
    "path_root = 'Data/basins'\n",
    "path_ts = os.path.join(path_root, 'time_series')\n",
    "path_attr = os.path.join(path_root, 'attributes')\n",
    "for p in [path_root, path_ts, path_attr]:\n",
    "    if os.path.isdir(p) is False:\n",
    "        os.mkdir(p)\n",
    "\n",
    "vote_gages = CRB_watersheds.id_gage.to_list()\n",
    "tokens = {}\n",
    "\n",
    "# get attribute names\n",
    "token = es.basin_token(-15176336368)\n",
    "attr = pd.DataFrame(token['static'], index=[0])\n",
    "attr_l = list(attr.columns)\n",
    "attr_l.insert(0, 'index')\n",
    "df = pd.DataFrame(columns = attr_l)\n",
    "\n",
    "# access VotE data\n",
    "for id_gage in vote_gages:\n",
    "    token = es.basin_token(id_gage, normalize_Q=False)\n",
    "\n",
    "    if token is not None:\n",
    "        tokens[id_gage] = token\n",
    "        # time series to netcdfs\n",
    "        sf = token['timeseries']\n",
    "        sf.set_index('date', inplace=True)\n",
    "        sf.index = pd.DatetimeIndex(sf.index)\n",
    "        sf.drop('q_quality', axis=1, inplace=True)\n",
    "        sf['dayofyear'] = sf.index.dayofyear\n",
    "        sf['year'] = sf.index.year\n",
    "        sf['month'] = sf.index.month\n",
    "        sf['day'] = sf.index.day\n",
    "        sf = sf.resample('D').mean()\n",
    "        sf.to_xarray().to_netcdf(os.path.join(path_ts, str(id_gage) + '.nc'))\n",
    "\n",
    "        # store the static attributes\n",
    "        b_attr = pd.DataFrame(token['static'], index=[0])\n",
    "        b_attr['index'] = id_gage\n",
    "        df = pd.concat([df, b_attr], ignore_index=True)\n",
    "df.to_csv(os.path.join(path_attr, 'all.csv'), index=False)\n",
    "df.to_file('Data/basins/attributes.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1aaed",
   "metadata": {},
   "source": [
    "### Combine Data with NID Data\n",
    "`VotE` data is combined with NID data to understand when and where there are operating reservoirs. Using the NID reservoir construction date, basins with reservoirs are separated into post-reservoir periods with a buffer of 5 years after the reservoir construction date to adjust for time needed to fill the reservoir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f246f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with NID data\n",
    "nid_df = gpd.read_file(\"Data/crb-nid-dams.gpkg\")\n",
    "CRB_watersheds = gpd.read_file('Data/attributes.gpkg')\n",
    "\n",
    "# point within polygon join\n",
    "nid_basins = gpd.sjoin(CRB_watersheds, nid_df, how='left', predicate='contains')\n",
    "nid_basins = nid_basins.groupby('index')['NID ID'].apply(list).reset_index(name='nid_dam')\n",
    "CRB_watersheds['nid_dam'] = nid_basins['nid_dam']\n",
    "\n",
    "static_df = pd.read_csv(\"Data/basins/attributes/all.csv\")\n",
    "\n",
    "# list of NID dams per basin\n",
    "for i in range(len(CRB_watersheds)):\n",
    "    if np.nan in CRB_watersheds.nid_dam[i]:\n",
    "        CRB_watersheds.nid_dam[i] = []\n",
    "\n",
    "# add number of NID dams variable\n",
    "CRB_watersheds['nid_n_dam'] = CRB_watersheds.nid_dam.apply(lambda x: len(x))\n",
    "\n",
    "# add year reservoir was completed variable\n",
    "static_df['yr_completed'] = np.nan\n",
    "for d in static_df['index']:\n",
    "    nid_list = CRB_watersheds.loc[CRB_watersheds['index'] == d, 'nid_dam'].item()\n",
    "    yr_list = nid_df.loc[nid_df['NID ID'].isin(nid_list), 'Year Completed'].tolist()\n",
    "    if ('' in yr_list) or (yr_list == []):\n",
    "        yr = np.nan # later remove dams with missing construction date\n",
    "    else:\n",
    "        yr = max([int(i) for i in yr_list])\n",
    "    static_df.loc[static_df['index'] == d, 'yr_completed'] = yr\n",
    "    \n",
    "# static_df.dropna(subset=['yr_completed'], inplace=True)\n",
    "\n",
    "# add reservoir volume variable\n",
    "static_df['nid_vol_c_yds'] = np.nan\n",
    "for d in static_df['index']:\n",
    "    nid_list = CRB_watersheds.loc[CRB_watersheds['index'] == d, 'nid_dam'].item()\n",
    "    vol_list = nid_df.loc[nid_df['NID ID'].isin(nid_list), 'Volume (Cubic Yards)'].tolist()\n",
    "    if '' in vol_list:\n",
    "        vol_total = np.nan # later add average volume\n",
    "    else:\n",
    "        vol_total = sum([int(i) for i in vol_list])\n",
    "    static_df.loc[static_df['index'] == d, 'nid_vol_c_yds'] = vol_total\n",
    "    \n",
    "static_df['nid_vol_c_yds'].fillna(static_df['nid_vol_c_yds'].mean(), inplace=True)\n",
    "\n",
    "# add reservoir name variable\n",
    "static_df['nid_name'] = np.nan\n",
    "\n",
    "for d in static_df['index']:\n",
    "    nid_list = CRB_watersheds.loc[CRB_watersheds['index'] == d, 'nid_dam'].item()\n",
    "    nam_list = nid_df.loc[nid_df['NID ID'].isin(nid_list), 'Dam Name'].tolist()\n",
    "    if '' in nam_list:\n",
    "        nam_str = '' # nid_name is '' for no reservoirs\n",
    "    else:\n",
    "        nam_str = ''\n",
    "        counter = 0\n",
    "        for i in nam_list:\n",
    "            if counter == len(nam_list) - 1:\n",
    "                n = ''\n",
    "            else:\n",
    "                n = ', '\n",
    "            counter += 1\n",
    "            nam_str += i + n # create string of reservoir names\n",
    "    static_df.loc[static_df['index'] == d, 'nid_name'] = nam_str\n",
    "    \n",
    "static_df['nid_name'].fillna('None', inplace=True)\n",
    "\n",
    "for d in static_df['index']:\n",
    "    nid_name = static_df.loc[static_df['index'] == d].nid_name\n",
    "    if nid_name.values[0] == '':\n",
    "        static_df.loc[static_df['index'] == d, 'yr_completed'] = 2023.0\n",
    "    else:\n",
    "        yr_completed = static_df.loc[static_df['index'] == d].yr_completed\n",
    "        if np.isnan(yr_completed.values[0]):\n",
    "            static_df.loc[static_df['index'] == d, 'yr_completed'] = 1900\n",
    "\n",
    "# static_df.to_csv('Data/basins/attributes-nid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-vote] *",
   "language": "python",
   "name": "conda-env-miniconda3-vote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
